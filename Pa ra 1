import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression as SklearnLogisticRegression
from sklearn.metrics import accuracy_score, classification_report

class LogisticRegressionFromScratch:
    def __init__(self, learning_rate=0.01, n_iters=1000):
        self.learning_rate = learning_rate
        self.n_iters = n_iters
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -250, 250)))
    
    def initialize_parameters(self, n_features):
        self.weights = np.random.normal(0, 0.01, n_features)
        self.bias = 0
    
    def compute_cost(self, y, y_pred):
        # Binary cross-entropy loss
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        cost = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))
        return cost
    
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.initialize_parameters(n_features)
        
        # Gradient descent
        for i in range(self.n_iters):
            # Forward pass
            linear_model = np.dot(X, self.weights) + self.bias
            y_pred = self.sigmoid(linear_model)
            
            # Compute cost
            cost = self.compute_cost(y, y_pred)
            self.cost_history.append(cost)
            
            # Backward pass (gradients)
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)
            
            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # Print cost every 100 iterations
            if i % 100 == 0:
                print(f"Iteration {i}, Cost: {cost:.4f}")
    
    def predict(self, X, threshold=0.5):
        linear_model = np.dot(X, self.weights) + self.bias
        y_pred = self.sigmoid(linear_model)
        return (y_pred >= threshold).astype(int)
    
    def predict_proba(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        return self.sigmoid(linear_model)

# Generate synthetic dataset for binary classification
print("Generating synthetic dataset...")
X, y = make_classification(
    n_samples=1000,
    n_features=2,
    n_redundant=0,
    n_informative=2,
    n_clusters_per_class=1,
    random_state=42
)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

# Initialize and train custom logistic regression
print("\nTraining Custom Logistic Regression...")
custom_lr = LogisticRegressionFromScratch(learning_rate=0.1, n_iters=1000)
custom_lr.fit(X_train, y_train)

# Make predictions with custom implementation
y_pred_custom = custom_lr.predict(X_test)
custom_accuracy = accuracy_score(y_test, y_pred_custom)

print(f"\nCustom Logistic Regression Accuracy: {custom_accuracy:.4f}")

# Compare with sklearn implementation
print("\nTraining Sklearn Logistic Regression for comparison...")
sklearn_lr = SklearnLogisticRegression(random_state=42)
sklearn_lr.fit(X_train, y_train)
y_pred_sklearn = sklearn_lr.predict(X_test)
sklearn_accuracy = accuracy_score(y_test, y_pred_sklearn)

print(f"Sklearn Logistic Regression Accuracy: {sklearn_accuracy:.4f}")

# Performance comparison
print("\n" + "="*50)
print("PERFORMANCE COMPARISON")
print("="*50)
print(f"Custom Implementation Accuracy: {custom_accuracy:.4f}")
print(f"Sklearn Implementation Accuracy: {sklearn_accuracy:.4f}")
print(f"Accuracy Difference: {abs(custom_accuracy - sklearn_accuracy):.4f}")

# Cost history visualization
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(custom_lr.cost_history)
plt.title('Cost History - Custom Logistic Regression')
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.grid(True, alpha=0.3)

# Decision boundary visualization
plt.subplot(1, 2, 2)
xx, yy = np.meshgrid(
    np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),
    np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100)
)
Z = custom_lr.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.RdYlBu, edgecolors='black')
plt.title('Decision Boundary - Custom Logistic Regression')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar()

plt.tight_layout()
plt.show()

# Detailed classification report
print("\n" + "="*50)
print("DETAILED CLASSIFICATION REPORT - CUSTOM IMPLEMENTATION")
print("="*50)
print(classification_report(y_test, y_pred_custom))

# Model parameters
print("\n" + "="*50)
print("MODEL PARAMETERS - CUSTOM IMPLEMENTATION")
print("="*50)
print(f"Weights: {custom_lr.weights}")
print(f"Bias: {custom_lr.bias:.4f}")
print(f"Final Cost: {custom_lr.cost_history[-1]:.4f}")
Generating synthetic dataset...
Training set shape: (800, 2)
Test set shape: (200, 2)

Training Custom Logistic Regression...
Iteration 0, Cost: 0.6931
Iteration 100, Cost: 0.3456
Iteration 200, Cost: 0.2874
...
Iteration 900, Cost: 0.2341

Custom Logistic Regression Accuracy: 0.8650

Training Sklearn Logistic Regression for comparison...
Sklearn Logistic Regression Accuracy: 0.8700

==================================================
PERFORMANCE COMPARISON
==================================================
Custom Implementation Accuracy: 0.8650
Sklearn Implementation Accuracy: 0.8700
Accuracy Difference: 0.0050
